{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d99f07f",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73ef4991",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from random import seed\n",
    "from random import randint\n",
    "import random\n",
    "from numpy.random import uniform\n",
    "from scipy.special import expit\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import datasets, transforms, models\n",
    "import torchvision\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9921bc4c",
   "metadata": {},
   "source": [
    "## Visualization & RNG setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "650bd2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  999\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "sns.set()\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "torch.backends.cudnn.deterministic = True # for deep learning CUDA library\n",
    "torch.backends.cudnn.benchmark = False # for deep learning CUDA library\n",
    "torch.cuda.manual_seed(manualSeed)\n",
    "torch.cuda.manual_seed_all(manualSeed) # if use multi-GPU\n",
    "np.random.seed(manualSeed) # for numpy-based backend, scikit-learn\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4159103",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa89bb3",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "The model performs binning in that it takes in snippets. Then, it creates masks per frame to compute spatial importance. Then, perform ConvLSTM to reconstruct an 'average image' per bin, and LSTM again. We should have weights being different between the 'averages' if we have an anomaly (put that in weights) and little difference otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21540cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from explainable_anomaly_detection import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f0b4de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = r'D:\\GSAI\\JUPYTER\\AI604\\UBI_FIGHTS'\n",
    "train_dataset = ViolenceDataset(DATA_PATH, sampling_freq=5)\n",
    "valid_dataset = ViolenceDataset(DATA_PATH, sampling_freq=5)\n",
    "test_dataset = ViolenceDataset(DATA_PATH, sampling_freq=5,is_test=True)\n",
    "\n",
    "# For train/valid split, we'll replace the actual video files with even split of normal and abnormal videos\n",
    "split_percent = 0.7\n",
    "normal_idx = np.arange(0,len(train_dataset.normal_videos)).astype('int')\n",
    "fight_idx = np.arange(0,len(train_dataset.fight_videos)).astype('int')\n",
    "np.random.shuffle(normal_idx)\n",
    "np.random.shuffle(fight_idx)\n",
    "normal_train = normal_idx[:int(split_percent*len(train_dataset.normal_videos))]\n",
    "normal_valid = normal_idx[int(split_percent*len(train_dataset.normal_videos)):]\n",
    "fight_train = fight_idx[:int(split_percent*len(train_dataset.fight_videos))]\n",
    "fight_valid = fight_idx[int(split_percent*len(train_dataset.fight_videos)):]\n",
    "\n",
    "train_dataset.normal_videos = [train_dataset.normal_videos[t] for t in normal_train]\n",
    "train_dataset.normal_labels = [train_dataset.normal_labels[t] for t in normal_train]\n",
    "train_dataset.fight_videos = [train_dataset.fight_videos[t] for t in fight_train]\n",
    "train_dataset.fight_labels = [train_dataset.fight_labels[t] for t in fight_train]\n",
    "\n",
    "valid_dataset.normal_videos = [valid_dataset.normal_videos[t] for t in normal_valid]\n",
    "valid_dataset.normal_labels = [valid_dataset.normal_labels[t] for t in normal_valid]\n",
    "valid_dataset.fight_videos = [valid_dataset.fight_videos[t] for t in fight_valid]\n",
    "valid_dataset.fight_labels = [valid_dataset.fight_labels[t] for t in fight_valid]\n",
    "\n",
    "train_dataset.actual_videos = train_dataset.fight_videos+train_dataset.normal_videos\n",
    "train_dataset.actual_labels = train_dataset.fight_labels+train_dataset.normal_labels\n",
    "valid_dataset.actual_videos = valid_dataset.fight_videos+valid_dataset.normal_videos\n",
    "valid_dataset.actual_labels = valid_dataset.fight_labels+valid_dataset.normal_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74ce03c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train & check performance\n",
    "n_epoch = 10\n",
    "valid_iter = 2\n",
    "lr = 1e-4\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8bf912b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SpatialBlockAbnormal = ConvSpaceMask().to(device)\n",
    "SpatialBlockNormal = ConvSpaceMask().to(device)\n",
    "TemporalBlockAbnormal = LSTMTimeMask().to(device)\n",
    "TemporalBlockNormal = LSTMTimeMask().to(device)\n",
    "\n",
    "net = CBAMVideoNetwork(SpatialBlockType=2, TemporalBlockType=2,kernel_size=3,n_channel=3).to(device)\n",
    "loss_fn = AttentionMatchLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23fb307c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size,shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bba7c8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/653 [00:18<48:47,  4.51s/it]  \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 528.00 MiB (GPU 0; 8.00 GiB total capacity; 5.50 GiB already allocated; 122.50 MiB free; 5.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m video \u001b[38;5;241m=\u001b[39m video\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     14\u001b[0m label \u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 15\u001b[0m abnormal_score, normal_score, temporal_attention_abnormal, temporal_attention_normal \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(label, abnormal_score,normal_score,temporal_attention_abnormal,temporal_attention_normal)\n\u001b[0;32m     17\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\GSAI\\JUPYTER\\AI604\\explainable_anomaly_detection.py:342\u001b[0m, in \u001b[0;36mCBAMVideoNetwork.forward\u001b[1;34m(self, input_tensor)\u001b[0m\n\u001b[0;32m    339\u001b[0m temporal_attention_abnormal_reshaped \u001b[38;5;241m=\u001b[39m reshape_attention(temporal_attention_abnormal,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_channel,H,W)\n\u001b[0;32m    340\u001b[0m temporal_attention_normal_reshaped \u001b[38;5;241m=\u001b[39m reshape_attention(temporal_attention_normal,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_channel,H,W)\n\u001b[1;32m--> 342\u001b[0m masked_tensor_abnormal \u001b[38;5;241m=\u001b[39m \u001b[43minput_tensor\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mspatial_attention_abnormal\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtemporal_attention_abnormal_reshaped\u001b[49m\n\u001b[0;32m    343\u001b[0m masked_tensor_normal \u001b[38;5;241m=\u001b[39m input_tensor\u001b[38;5;241m*\u001b[39mspatial_attention_normal\u001b[38;5;241m*\u001b[39mtemporal_attention_normal_reshaped\n\u001b[0;32m    345\u001b[0m abnormal_sum \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((batch_size,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_channel,masked_tensor_abnormal\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m],masked_tensor_abnormal\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m3\u001b[39m]))\u001b[38;5;241m.\u001b[39mto(input_tensor\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 528.00 MiB (GPU 0; 8.00 GiB total capacity; 5.50 GiB already allocated; 122.50 MiB free; 5.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "best_valid_loss = 1e10\n",
    "train_loss_list = []\n",
    "valid_loss_list = []\n",
    "test_loss_list = []\n",
    "for n in range(1,n_epoch+1):\n",
    "    print(f'Epoch {n}:')\n",
    "    train_loss = 0\n",
    "    valid_loss = 0\n",
    "    test_loss = 0\n",
    "    net.train()\n",
    "    print('Training...')\n",
    "    for video,label in tqdm(iter(train_dataloader)):\n",
    "        video = video.to(device)\n",
    "        label = label.to(device)\n",
    "        abnormal_score, normal_score, temporal_attention_abnormal, temporal_attention_normal = net(video.to(device))\n",
    "        loss = loss_fn(label, abnormal_score,normal_score,temporal_attention_abnormal,temporal_attention_normal)\n",
    "        train_loss += loss.cpu().detach().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        del video, label, abnormal_score, normal_score, temporal_attention_abnormal, temporal_attention_normal, loss\n",
    "    \n",
    "    net.eval()\n",
    "    if n % valid_iter == 0:\n",
    "        print('Validation...')\n",
    "        for video,label in tqdm(iter(valid_dataloader)):\n",
    "            video = video.to(device)\n",
    "            label = label.to(device)\n",
    "            abnormal_score, normal_score, temporal_attention_abnormal, temporal_attention_normal = net(video)\n",
    "            loss = loss_fn(label, abnormal_score,normal_score,temporal_attention_abnormal,temporal_attention_normal)\n",
    "            valid_loss += loss.cpu().detach().item()\n",
    "\n",
    "            del video, label, abnormal_score, normal_score, temporal_attention_abnormal, temporal_attention_normal, loss    \n",
    "\n",
    "    print('Testing...')\n",
    "    for video,label in tqdm(iter(test_dataloader)):\n",
    "        video = video.to(device)\n",
    "        label = label.to(device)\n",
    "        abnormal_score, normal_score, temporal_attention_abnormal, temporal_attention_normal = net(video.to(device))\n",
    "        loss = loss_fn(label, abnormal_score,normal_score,temporal_attention_abnormal,temporal_attention_normal)\n",
    "        test_loss += loss.cpu().detach().item()\n",
    "        \n",
    "        del video, label, abnormal_score, normal_score, temporal_attention_abnormal, temporal_attention_normal, loss\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        torch.save(f'net_epoch_{n}.pt',net.state_dict())\n",
    "    \n",
    "    train_loss_list.append(train_loss)\n",
    "    valid_loss_list.append(valid_loss)\n",
    "    test_loss_list.append(test_loss)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
